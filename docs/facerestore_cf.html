
<!DOCTYPE html>
<html>
<head>

<meta charset="UTF-8">

<title>
    Bandit Report
</title>

<style>

html * {
    font-family: "Arial", sans-serif;
}

pre {
    font-family: "Monaco", monospace;
}

.bordered-box {
    border: 1px solid black;
    padding-top:.5em;
    padding-bottom:.5em;
    padding-left:1em;
}

.metrics-box {
    font-size: 1.1em;
    line-height: 130%;
}

.metrics-title {
    font-size: 1.5em;
    font-weight: 500;
    margin-bottom: .25em;
}

.issue-description {
    font-size: 1.3em;
    font-weight: 500;
}

.candidate-issues {
    margin-left: 2em;
    border-left: solid 1px; LightGray;
    padding-left: 5%;
    margin-top: .2em;
    margin-bottom: .2em;
}

.issue-block {
    border: 1px solid LightGray;
    padding-left: .5em;
    padding-top: .5em;
    padding-bottom: .5em;
    margin-bottom: .5em;
}

.issue-sev-high {
    background-color: Pink;
}

.issue-sev-medium {
    background-color: NavajoWhite;
}

.issue-sev-low {
    background-color: LightCyan;
}

</style>
</head>

<body>

<div id="metrics">
    <div class="metrics-box bordered-box">
        <div class="metrics-title">
            Metrics:<br>
        </div>
        Total lines of code: <span id="loc">8163</span><br>
        Total lines skipped (#nosec): <span id="nosec">0</span>
    </div>
</div>




<br>
<div id="results">
    
<div id="issue-0">
<div class="issue-block issue-sev-low">
    <b>blacklist: </b> Consider possible security implications associated with the subprocess module.<br>
    <b>Test ID:</b> B404<br>
    <b>Severity: </b>LOW<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py</a><br>
    <b>Line number: </b>6<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_imports.html#b404-import-subprocess" target="_blank">https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_imports.html#b404-import-subprocess</a><br>

<div class="code">
<pre>
1	#!/usr/bin/env python
2	
3	from setuptools import find_packages, setup
4	
5	import os
6	import subprocess
7	import sys
8	import time
9	import torch
10	from torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension
11	
12	version_file = &#x27;./basicsr/version.py&#x27;
13	
14	
15	def readme():
16	    with open(&#x27;README.md&#x27;, encoding=&#x27;utf-8&#x27;) as f:
17	        content = f.read()
18	    return content
19	
20	
21	def get_git_hash():
22	
23	    def _minimal_ext_cmd(cmd):
24	        # construct minimal environment
25	        env = {}
26	        for k in [&#x27;SYSTEMROOT&#x27;, &#x27;PATH&#x27;, &#x27;HOME&#x27;]:
27	            v = os.environ.get(k)
28	            if v is not None:
29	                env[k] = v
30	        # LANGUAGE is used on win32
31	        env[&#x27;LANGUAGE&#x27;] = &#x27;C&#x27;
32	        env[&#x27;LANG&#x27;] = &#x27;C&#x27;
33	        env[&#x27;LC_ALL&#x27;] = &#x27;C&#x27;
34	        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]
35	        return out
36	
37	    try:
38	        out = _minimal_ext_cmd([&#x27;git&#x27;, &#x27;rev-parse&#x27;, &#x27;HEAD&#x27;])
39	        sha = out.strip().decode(&#x27;ascii&#x27;)
40	    except OSError:
41	        sha = &#x27;unknown&#x27;
42	
43	    return sha
44	
45	
46	def get_hash():
47	    if os.path.exists(&#x27;.git&#x27;):
48	        sha = get_git_hash()[:7]
49	    elif os.path.exists(version_file):
50	        try:
51	            from version import __version__
52	            sha = __version__.split(&#x27;+&#x27;)[-1]
53	        except ImportError:
54	            raise ImportError(&#x27;Unable to get git version&#x27;)
55	    else:
56	        sha = &#x27;unknown&#x27;
57	
58	    return sha
59	
60	
61	def write_version_py():
62	    content = &quot;&quot;&quot;# GENERATED VERSION FILE
63	# TIME: {}
64	__version__ = &#x27;{}&#x27;
65	__gitsha__ = &#x27;{}&#x27;
66	version_info = ({})
67	&quot;&quot;&quot;
68	    sha = get_hash()
69	    with open(&#x27;./basicsr/VERSION&#x27;, &#x27;r&#x27;) as f:
70	        SHORT_VERSION = f.read().strip()
71	    VERSION_INFO = &#x27;, &#x27;.join([x if x.isdigit() else f&#x27;&quot;{x}&quot;&#x27; for x in SHORT_VERSION.split(&#x27;.&#x27;)])
72	
73	    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)
74	    with open(version_file, &#x27;w&#x27;) as f:
75	        f.write(version_file_str)
76	
77	
78	def get_version():
79	    with open(version_file, &#x27;r&#x27;) as f:
80	        exec(compile(f.read(), version_file, &#x27;exec&#x27;))
81	    return locals()[&#x27;__version__&#x27;]
82	
83	
84	def make_cuda_ext(name, module, sources, sources_cuda=None):
85	    if sources_cuda is None:
86	        sources_cuda = []
87	    define_macros = []
88	    extra_compile_args = {&#x27;cxx&#x27;: []}
89	
90	    if torch.cuda.is_available() or os.getenv(&#x27;FORCE_CUDA&#x27;, &#x27;0&#x27;) == &#x27;1&#x27;:
91	        define_macros += [(&#x27;WITH_CUDA&#x27;, None)]
92	        extension = CUDAExtension
93	        extra_compile_args[&#x27;nvcc&#x27;] = [
94	            &#x27;-D__CUDA_NO_HALF_OPERATORS__&#x27;,
95	            &#x27;-D__CUDA_NO_HALF_CONVERSIONS__&#x27;,
96	            &#x27;-D__CUDA_NO_HALF2_OPERATORS__&#x27;,
97	        ]
98	        sources += sources_cuda
99	    else:
100	        print(f&#x27;Compiling {name} without CUDA&#x27;)
101	        extension = CppExtension
102	
103	    return extension(
104	        name=f&#x27;{module}.{name}&#x27;,
105	        sources=[os.path.join(*module.split(&#x27;.&#x27;), p) for p in sources],
106	        define_macros=define_macros,
107	        extra_compile_args=extra_compile_args)
108	
109	
110	def get_requirements(filename=&#x27;requirements.txt&#x27;):
111	    with open(os.path.join(&#x27;.&#x27;, filename), &#x27;r&#x27;) as f:
112	        requires = [line.replace(&#x27;\n&#x27;, &#x27;&#x27;) for line in f.readlines()]
113	    return requires
114	
115	
116	if __name__ == &#x27;__main__&#x27;:
117	    if &#x27;--cuda_ext&#x27; in sys.argv:
118	        ext_modules = [
119	            make_cuda_ext(
120	                name=&#x27;deform_conv_ext&#x27;,
</pre>
</div>


</div>
</div>

<div id="issue-1">
<div class="issue-block issue-sev-low">
    <b>subprocess_without_shell_equals_true: </b> subprocess call - check for execution of untrusted input.<br>
    <b>Test ID:</b> B603<br>
    <b>Severity: </b>LOW<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py</a><br>
    <b>Line number: </b>34<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/plugins/b603_subprocess_without_shell_equals_true.html" target="_blank">https://bandit.readthedocs.io/en/1.7.9/plugins/b603_subprocess_without_shell_equals_true.html</a><br>

<div class="code">
<pre>
1	#!/usr/bin/env python
2	
3	from setuptools import find_packages, setup
4	
5	import os
6	import subprocess
7	import sys
8	import time
9	import torch
10	from torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension
11	
12	version_file = &#x27;./basicsr/version.py&#x27;
13	
14	
15	def readme():
16	    with open(&#x27;README.md&#x27;, encoding=&#x27;utf-8&#x27;) as f:
17	        content = f.read()
18	    return content
19	
20	
21	def get_git_hash():
22	
23	    def _minimal_ext_cmd(cmd):
24	        # construct minimal environment
25	        env = {}
26	        for k in [&#x27;SYSTEMROOT&#x27;, &#x27;PATH&#x27;, &#x27;HOME&#x27;]:
27	            v = os.environ.get(k)
28	            if v is not None:
29	                env[k] = v
30	        # LANGUAGE is used on win32
31	        env[&#x27;LANGUAGE&#x27;] = &#x27;C&#x27;
32	        env[&#x27;LANG&#x27;] = &#x27;C&#x27;
33	        env[&#x27;LC_ALL&#x27;] = &#x27;C&#x27;
34	        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]
35	        return out
36	
37	    try:
38	        out = _minimal_ext_cmd([&#x27;git&#x27;, &#x27;rev-parse&#x27;, &#x27;HEAD&#x27;])
39	        sha = out.strip().decode(&#x27;ascii&#x27;)
40	    except OSError:
41	        sha = &#x27;unknown&#x27;
42	
43	    return sha
44	
45	
46	def get_hash():
47	    if os.path.exists(&#x27;.git&#x27;):
48	        sha = get_git_hash()[:7]
49	    elif os.path.exists(version_file):
50	        try:
51	            from version import __version__
52	            sha = __version__.split(&#x27;+&#x27;)[-1]
53	        except ImportError:
54	            raise ImportError(&#x27;Unable to get git version&#x27;)
55	    else:
56	        sha = &#x27;unknown&#x27;
57	
58	    return sha
59	
60	
61	def write_version_py():
62	    content = &quot;&quot;&quot;# GENERATED VERSION FILE
63	# TIME: {}
64	__version__ = &#x27;{}&#x27;
65	__gitsha__ = &#x27;{}&#x27;
66	version_info = ({})
67	&quot;&quot;&quot;
68	    sha = get_hash()
69	    with open(&#x27;./basicsr/VERSION&#x27;, &#x27;r&#x27;) as f:
70	        SHORT_VERSION = f.read().strip()
71	    VERSION_INFO = &#x27;, &#x27;.join([x if x.isdigit() else f&#x27;&quot;{x}&quot;&#x27; for x in SHORT_VERSION.split(&#x27;.&#x27;)])
72	
73	    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)
74	    with open(version_file, &#x27;w&#x27;) as f:
75	        f.write(version_file_str)
76	
77	
78	def get_version():
79	    with open(version_file, &#x27;r&#x27;) as f:
80	        exec(compile(f.read(), version_file, &#x27;exec&#x27;))
81	    return locals()[&#x27;__version__&#x27;]
82	
83	
84	def make_cuda_ext(name, module, sources, sources_cuda=None):
85	    if sources_cuda is None:
86	        sources_cuda = []
87	    define_macros = []
88	    extra_compile_args = {&#x27;cxx&#x27;: []}
89	
90	    if torch.cuda.is_available() or os.getenv(&#x27;FORCE_CUDA&#x27;, &#x27;0&#x27;) == &#x27;1&#x27;:
91	        define_macros += [(&#x27;WITH_CUDA&#x27;, None)]
92	        extension = CUDAExtension
93	        extra_compile_args[&#x27;nvcc&#x27;] = [
94	            &#x27;-D__CUDA_NO_HALF_OPERATORS__&#x27;,
95	            &#x27;-D__CUDA_NO_HALF_CONVERSIONS__&#x27;,
96	            &#x27;-D__CUDA_NO_HALF2_OPERATORS__&#x27;,
97	        ]
98	        sources += sources_cuda
99	    else:
100	        print(f&#x27;Compiling {name} without CUDA&#x27;)
101	        extension = CppExtension
102	
103	    return extension(
104	        name=f&#x27;{module}.{name}&#x27;,
105	        sources=[os.path.join(*module.split(&#x27;.&#x27;), p) for p in sources],
106	        define_macros=define_macros,
107	        extra_compile_args=extra_compile_args)
108	
109	
110	def get_requirements(filename=&#x27;requirements.txt&#x27;):
111	    with open(os.path.join(&#x27;.&#x27;, filename), &#x27;r&#x27;) as f:
112	        requires = [line.replace(&#x27;\n&#x27;, &#x27;&#x27;) for line in f.readlines()]
113	    return requires
114	
115	
116	if __name__ == &#x27;__main__&#x27;:
117	    if &#x27;--cuda_ext&#x27; in sys.argv:
118	        ext_modules = [
119	            make_cuda_ext(
120	                name=&#x27;deform_conv_ext&#x27;,
</pre>
</div>


</div>
</div>

<div id="issue-2">
<div class="issue-block issue-sev-medium">
    <b>exec_used: </b> Use of exec detected.<br>
    <b>Test ID:</b> B102<br>
    <b>Severity: </b>MEDIUM<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/setup.py</a><br>
    <b>Line number: </b>80<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/plugins/b102_exec_used.html" target="_blank">https://bandit.readthedocs.io/en/1.7.9/plugins/b102_exec_used.html</a><br>

<div class="code">
<pre>
20	
21	def get_git_hash():
22	
23	    def _minimal_ext_cmd(cmd):
24	        # construct minimal environment
25	        env = {}
26	        for k in [&#x27;SYSTEMROOT&#x27;, &#x27;PATH&#x27;, &#x27;HOME&#x27;]:
27	            v = os.environ.get(k)
28	            if v is not None:
29	                env[k] = v
30	        # LANGUAGE is used on win32
31	        env[&#x27;LANGUAGE&#x27;] = &#x27;C&#x27;
32	        env[&#x27;LANG&#x27;] = &#x27;C&#x27;
33	        env[&#x27;LC_ALL&#x27;] = &#x27;C&#x27;
34	        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]
35	        return out
36	
37	    try:
38	        out = _minimal_ext_cmd([&#x27;git&#x27;, &#x27;rev-parse&#x27;, &#x27;HEAD&#x27;])
39	        sha = out.strip().decode(&#x27;ascii&#x27;)
40	    except OSError:
41	        sha = &#x27;unknown&#x27;
42	
43	    return sha
44	
45	
46	def get_hash():
47	    if os.path.exists(&#x27;.git&#x27;):
48	        sha = get_git_hash()[:7]
49	    elif os.path.exists(version_file):
50	        try:
51	            from version import __version__
52	            sha = __version__.split(&#x27;+&#x27;)[-1]
53	        except ImportError:
54	            raise ImportError(&#x27;Unable to get git version&#x27;)
55	    else:
56	        sha = &#x27;unknown&#x27;
57	
58	    return sha
59	
60	
61	def write_version_py():
62	    content = &quot;&quot;&quot;# GENERATED VERSION FILE
63	# TIME: {}
64	__version__ = &#x27;{}&#x27;
65	__gitsha__ = &#x27;{}&#x27;
66	version_info = ({})
67	&quot;&quot;&quot;
68	    sha = get_hash()
69	    with open(&#x27;./basicsr/VERSION&#x27;, &#x27;r&#x27;) as f:
70	        SHORT_VERSION = f.read().strip()
71	    VERSION_INFO = &#x27;, &#x27;.join([x if x.isdigit() else f&#x27;&quot;{x}&quot;&#x27; for x in SHORT_VERSION.split(&#x27;.&#x27;)])
72	
73	    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)
74	    with open(version_file, &#x27;w&#x27;) as f:
75	        f.write(version_file_str)
76	
77	
78	def get_version():
79	    with open(version_file, &#x27;r&#x27;) as f:
80	        exec(compile(f.read(), version_file, &#x27;exec&#x27;))
81	    return locals()[&#x27;__version__&#x27;]
82	
83	
84	def make_cuda_ext(name, module, sources, sources_cuda=None):
85	    if sources_cuda is None:
86	        sources_cuda = []
87	    define_macros = []
88	    extra_compile_args = {&#x27;cxx&#x27;: []}
89	
90	    if torch.cuda.is_available() or os.getenv(&#x27;FORCE_CUDA&#x27;, &#x27;0&#x27;) == &#x27;1&#x27;:
91	        define_macros += [(&#x27;WITH_CUDA&#x27;, None)]
92	        extension = CUDAExtension
93	        extra_compile_args[&#x27;nvcc&#x27;] = [
94	            &#x27;-D__CUDA_NO_HALF_OPERATORS__&#x27;,
95	            &#x27;-D__CUDA_NO_HALF_CONVERSIONS__&#x27;,
96	            &#x27;-D__CUDA_NO_HALF2_OPERATORS__&#x27;,
97	        ]
98	        sources += sources_cuda
99	    else:
100	        print(f&#x27;Compiling {name} without CUDA&#x27;)
101	        extension = CppExtension
102	
103	    return extension(
104	        name=f&#x27;{module}.{name}&#x27;,
105	        sources=[os.path.join(*module.split(&#x27;.&#x27;), p) for p in sources],
106	        define_macros=define_macros,
107	        extra_compile_args=extra_compile_args)
108	
109	
110	def get_requirements(filename=&#x27;requirements.txt&#x27;):
111	    with open(os.path.join(&#x27;.&#x27;, filename), &#x27;r&#x27;) as f:
112	        requires = [line.replace(&#x27;\n&#x27;, &#x27;&#x27;) for line in f.readlines()]
113	    return requires
114	
115	
116	if __name__ == &#x27;__main__&#x27;:
117	    if &#x27;--cuda_ext&#x27; in sys.argv:
118	        ext_modules = [
119	            make_cuda_ext(
120	                name=&#x27;deform_conv_ext&#x27;,
121	                module=&#x27;ops.dcn&#x27;,
122	                sources=[&#x27;src/deform_conv_ext.cpp&#x27;],
123	                sources_cuda=[&#x27;src/deform_conv_cuda.cpp&#x27;, &#x27;src/deform_conv_cuda_kernel.cu&#x27;]),
124	            make_cuda_ext(
125	                name=&#x27;fused_act_ext&#x27;,
126	                module=&#x27;ops.fused_act&#x27;,
127	                sources=[&#x27;src/fused_bias_act.cpp&#x27;],
128	                sources_cuda=[&#x27;src/fused_bias_act_kernel.cu&#x27;]),
129	            make_cuda_ext(
130	                name=&#x27;upfirdn2d_ext&#x27;,
131	                module=&#x27;ops.upfirdn2d&#x27;,
132	                sources=[&#x27;src/upfirdn2d.cpp&#x27;],
133	                sources_cuda=[&#x27;src/upfirdn2d_kernel.cu&#x27;]),
134	        ]
135	        sys.argv.remove(&#x27;--cuda_ext&#x27;)
136	    else:
137	        ext_modules = []
138	
139	    write_version_py()
</pre>
</div>


</div>
</div>

<div id="issue-3">
<div class="issue-block issue-sev-low">
    <b>blacklist: </b> Consider possible security implications associated with the subprocess module.<br>
    <b>Test ID:</b> B404<br>
    <b>Severity: </b>LOW<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/dist_util.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/dist_util.py</a><br>
    <b>Line number: </b>4<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_imports.html#b404-import-subprocess" target="_blank">https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_imports.html#b404-import-subprocess</a><br>

<div class="code">
<pre>
1	# Modified from https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/dist_utils.py  # noqa: E501
2	import functools
3	import os
4	import subprocess
5	import torch
6	import torch.distributed as dist
7	import torch.multiprocessing as mp
8	
9	
10	def init_dist(launcher, backend=&#x27;nccl&#x27;, **kwargs):
11	    if mp.get_start_method(allow_none=True) is None:
12	        mp.set_start_method(&#x27;spawn&#x27;)
13	    if launcher == &#x27;pytorch&#x27;:
14	        _init_dist_pytorch(backend, **kwargs)
15	    elif launcher == &#x27;slurm&#x27;:
16	        _init_dist_slurm(backend, **kwargs)
17	    else:
18	        raise ValueError(f&#x27;Invalid launcher type: {launcher}&#x27;)
19	
20	
21	def _init_dist_pytorch(backend, **kwargs):
22	    rank = int(os.environ[&#x27;RANK&#x27;])
23	    num_gpus = torch.cuda.device_count()
24	    torch.cuda.set_device(rank % num_gpus)
25	    dist.init_process_group(backend=backend, **kwargs)
26	
27	
28	def _init_dist_slurm(backend, port=None):
29	    &quot;&quot;&quot;Initialize slurm distributed training environment.
30	
31	    If argument ``port`` is not specified, then the master port will be system
32	    environment variable ``MASTER_PORT``. If ``MASTER_PORT`` is not in system
33	    environment variable, then a default port ``29500`` will be used.
34	
35	    Args:
36	        backend (str): Backend of torch.distributed.
37	        port (int, optional): Master port. Defaults to None.
38	    &quot;&quot;&quot;
39	    proc_id = int(os.environ[&#x27;SLURM_PROCID&#x27;])
40	    ntasks = int(os.environ[&#x27;SLURM_NTASKS&#x27;])
41	    node_list = os.environ[&#x27;SLURM_NODELIST&#x27;]
42	    num_gpus = torch.cuda.device_count()
43	    torch.cuda.set_device(proc_id % num_gpus)
44	    addr = subprocess.getoutput(f&#x27;scontrol show hostname {node_list} | head -n1&#x27;)
45	    # specify master port
46	    if port is not None:
47	        os.environ[&#x27;MASTER_PORT&#x27;] = str(port)
48	    elif &#x27;MASTER_PORT&#x27; in os.environ:
49	        pass  # use MASTER_PORT in the environment variable
50	    else:
51	        # 29500 is torch.distributed default port
52	        os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;29500&#x27;
53	    os.environ[&#x27;MASTER_ADDR&#x27;] = addr
54	    os.environ[&#x27;WORLD_SIZE&#x27;] = str(ntasks)
55	    os.environ[&#x27;LOCAL_RANK&#x27;] = str(proc_id % num_gpus)
56	    os.environ[&#x27;RANK&#x27;] = str(proc_id)
57	    dist.init_process_group(backend=backend)
58	
59	
60	def get_dist_info():
61	    if dist.is_available():
62	        initialized = dist.is_initialized()
63	    else:
64	        initialized = False
65	    if initialized:
66	        rank = dist.get_rank()
67	        world_size = dist.get_world_size()
68	    else:
69	        rank = 0
70	        world_size = 1
71	    return rank, world_size
72	
73	
74	def master_only(func):
75	
76	    @functools.wraps(func)
77	    def wrapper(*args, **kwargs):
78	        rank, _ = get_dist_info()
79	        if rank == 0:
80	            return func(*args, **kwargs)
81	
82	    return wrapper
</pre>
</div>


</div>
</div>

<div id="issue-4">
<div class="issue-block issue-sev-high">
    <b>start_process_with_a_shell: </b> Starting a process with a shell, possible injection detected, security issue.<br>
    <b>Test ID:</b> B605<br>
    <b>Severity: </b>HIGH<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/dist_util.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/dist_util.py</a><br>
    <b>Line number: </b>44<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/plugins/b605_start_process_with_a_shell.html" target="_blank">https://bandit.readthedocs.io/en/1.7.9/plugins/b605_start_process_with_a_shell.html</a><br>

<div class="code">
<pre>
1	# Modified from https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/dist_utils.py  # noqa: E501
2	import functools
3	import os
4	import subprocess
5	import torch
6	import torch.distributed as dist
7	import torch.multiprocessing as mp
8	
9	
10	def init_dist(launcher, backend=&#x27;nccl&#x27;, **kwargs):
11	    if mp.get_start_method(allow_none=True) is None:
12	        mp.set_start_method(&#x27;spawn&#x27;)
13	    if launcher == &#x27;pytorch&#x27;:
14	        _init_dist_pytorch(backend, **kwargs)
15	    elif launcher == &#x27;slurm&#x27;:
16	        _init_dist_slurm(backend, **kwargs)
17	    else:
18	        raise ValueError(f&#x27;Invalid launcher type: {launcher}&#x27;)
19	
20	
21	def _init_dist_pytorch(backend, **kwargs):
22	    rank = int(os.environ[&#x27;RANK&#x27;])
23	    num_gpus = torch.cuda.device_count()
24	    torch.cuda.set_device(rank % num_gpus)
25	    dist.init_process_group(backend=backend, **kwargs)
26	
27	
28	def _init_dist_slurm(backend, port=None):
29	    &quot;&quot;&quot;Initialize slurm distributed training environment.
30	
31	    If argument ``port`` is not specified, then the master port will be system
32	    environment variable ``MASTER_PORT``. If ``MASTER_PORT`` is not in system
33	    environment variable, then a default port ``29500`` will be used.
34	
35	    Args:
36	        backend (str): Backend of torch.distributed.
37	        port (int, optional): Master port. Defaults to None.
38	    &quot;&quot;&quot;
39	    proc_id = int(os.environ[&#x27;SLURM_PROCID&#x27;])
40	    ntasks = int(os.environ[&#x27;SLURM_NTASKS&#x27;])
41	    node_list = os.environ[&#x27;SLURM_NODELIST&#x27;]
42	    num_gpus = torch.cuda.device_count()
43	    torch.cuda.set_device(proc_id % num_gpus)
44	    addr = subprocess.getoutput(f&#x27;scontrol show hostname {node_list} | head -n1&#x27;)
45	    # specify master port
46	    if port is not None:
47	        os.environ[&#x27;MASTER_PORT&#x27;] = str(port)
48	    elif &#x27;MASTER_PORT&#x27; in os.environ:
49	        pass  # use MASTER_PORT in the environment variable
50	    else:
51	        # 29500 is torch.distributed default port
52	        os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;29500&#x27;
53	    os.environ[&#x27;MASTER_ADDR&#x27;] = addr
54	    os.environ[&#x27;WORLD_SIZE&#x27;] = str(ntasks)
55	    os.environ[&#x27;LOCAL_RANK&#x27;] = str(proc_id % num_gpus)
56	    os.environ[&#x27;RANK&#x27;] = str(proc_id)
57	    dist.init_process_group(backend=backend)
58	
59	
60	def get_dist_info():
61	    if dist.is_available():
62	        initialized = dist.is_initialized()
63	    else:
64	        initialized = False
65	    if initialized:
66	        rank = dist.get_rank()
67	        world_size = dist.get_world_size()
68	    else:
69	        rank = 0
70	        world_size = 1
71	    return rank, world_size
72	
73	
74	def master_only(func):
75	
76	    @functools.wraps(func)
77	    def wrapper(*args, **kwargs):
78	        rank, _ = get_dist_info()
79	        if rank == 0:
80	            return func(*args, **kwargs)
81	
82	    return wrapper
</pre>
</div>


</div>
</div>

<div id="issue-5">
<div class="issue-block issue-sev-medium">
    <b>yaml_load: </b> Use of unsafe yaml load. Allows instantiation of arbitrary objects. Consider yaml.safe_load().<br>
    <b>Test ID:</b> B506<br>
    <b>Severity: </b>MEDIUM<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/20.html" target="_blank">CWE-20</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/options.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/basicsr/utils/options.py</a><br>
    <b>Line number: </b>44<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/plugins/b506_yaml_load.html" target="_blank">https://bandit.readthedocs.io/en/1.7.9/plugins/b506_yaml_load.html</a><br>

<div class="code">
<pre>
1	import yaml
2	import time
3	from collections import OrderedDict
4	from os import path as osp
5	from custom_nodes.facerestore_cf.basicsr.utils.misc import get_time_str
6	
7	def ordered_yaml():
8	    &quot;&quot;&quot;Support OrderedDict for yaml.
9	
10	    Returns:
11	        yaml Loader and Dumper.
12	    &quot;&quot;&quot;
13	    try:
14	        from yaml import CDumper as Dumper
15	        from yaml import CLoader as Loader
16	    except ImportError:
17	        from yaml import Dumper, Loader
18	
19	    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG
20	
21	    def dict_representer(dumper, data):
22	        return dumper.represent_dict(data.items())
23	
24	    def dict_constructor(loader, node):
25	        return OrderedDict(loader.construct_pairs(node))
26	
27	    Dumper.add_representer(OrderedDict, dict_representer)
28	    Loader.add_constructor(_mapping_tag, dict_constructor)
29	    return Loader, Dumper
30	
31	
32	def parse(opt_path, root_path, is_train=True):
33	    &quot;&quot;&quot;Parse option file.
34	
35	    Args:
36	        opt_path (str): Option file path.
37	        is_train (str): Indicate whether in training or not. Default: True.
38	
39	    Returns:
40	        (dict): Options.
41	    &quot;&quot;&quot;
42	    with open(opt_path, mode=&#x27;r&#x27;) as f:
43	        Loader, _ = ordered_yaml()
44	        opt = yaml.load(f, Loader=Loader)
45	
46	    opt[&#x27;is_train&#x27;] = is_train
47	
48	    # opt[&#x27;name&#x27;] = f&quot;{get_time_str()}_{opt[&#x27;name&#x27;]}&quot;
49	    if opt[&#x27;path&#x27;].get(&#x27;resume_state&#x27;, None): # Shangchen added
50	        resume_state_path = opt[&#x27;path&#x27;].get(&#x27;resume_state&#x27;)
51	        opt[&#x27;name&#x27;] = resume_state_path.split(&quot;/&quot;)[-3]
52	    else:
53	        opt[&#x27;name&#x27;] = f&quot;{get_time_str()}_{opt[&#x27;name&#x27;]}&quot;
54	
55	
56	    # datasets
57	    for phase, dataset in opt[&#x27;datasets&#x27;].items():
58	        # for several datasets, e.g., test_1, test_2
59	        phase = phase.split(&#x27;_&#x27;)[0]
60	        dataset[&#x27;phase&#x27;] = phase
61	        if &#x27;scale&#x27; in opt:
62	            dataset[&#x27;scale&#x27;] = opt[&#x27;scale&#x27;]
63	        if dataset.get(&#x27;dataroot_gt&#x27;) is not None:
64	            dataset[&#x27;dataroot_gt&#x27;] = osp.expanduser(dataset[&#x27;dataroot_gt&#x27;])
65	        if dataset.get(&#x27;dataroot_lq&#x27;) is not None:
66	            dataset[&#x27;dataroot_lq&#x27;] = osp.expanduser(dataset[&#x27;dataroot_lq&#x27;])
67	
68	    # paths
69	    for key, val in opt[&#x27;path&#x27;].items():
70	        if (val is not None) and (&#x27;resume_state&#x27; in key or &#x27;pretrain_network&#x27; in key):
71	            opt[&#x27;path&#x27;][key] = osp.expanduser(val)
72	
73	    if is_train:
74	        experiments_root = osp.join(root_path, &#x27;experiments&#x27;, opt[&#x27;name&#x27;])
75	        opt[&#x27;path&#x27;][&#x27;experiments_root&#x27;] = experiments_root
76	        opt[&#x27;path&#x27;][&#x27;models&#x27;] = osp.join(experiments_root, &#x27;models&#x27;)
77	        opt[&#x27;path&#x27;][&#x27;training_states&#x27;] = osp.join(experiments_root, &#x27;training_states&#x27;)
78	        opt[&#x27;path&#x27;][&#x27;log&#x27;] = experiments_root
79	        opt[&#x27;path&#x27;][&#x27;visualization&#x27;] = osp.join(experiments_root, &#x27;visualization&#x27;)
80	
81	    else:  # test
82	        results_root = osp.join(root_path, &#x27;results&#x27;, opt[&#x27;name&#x27;])
83	        opt[&#x27;path&#x27;][&#x27;results_root&#x27;] = results_root
84	        opt[&#x27;path&#x27;][&#x27;log&#x27;] = results_root
85	        opt[&#x27;path&#x27;][&#x27;visualization&#x27;] = osp.join(results_root, &#x27;visualization&#x27;)
86	
87	    return opt
88	
89	
90	def dict2str(opt, indent_level=1):
91	    &quot;&quot;&quot;dict to string for printing options.
92	
93	    Args:
94	        opt (dict): Option dict.
95	        indent_level (int): Indent level. Default: 1.
96	
97	    Return:
98	        (str): Option string for printing.
99	    &quot;&quot;&quot;
100	    msg = &#x27;\n&#x27;
101	    for k, v in opt.items():
102	        if isinstance(v, dict):
103	            msg += &#x27; &#x27; * (indent_level * 2) + k + &#x27;:[&#x27;
104	            msg += dict2str(v, indent_level + 1)
105	            msg += &#x27; &#x27; * (indent_level * 2) + &#x27;]\n&#x27;
106	        else:
107	            msg += &#x27; &#x27; * (indent_level * 2) + k + &#x27;: &#x27; + str(v) + &#x27;\n&#x27;
108	    return msg
</pre>
</div>


</div>
</div>

<div id="issue-6">
<div class="issue-block issue-sev-medium">
    <b>blacklist: </b> Use of possibly insecure function - consider using safer ast.literal_eval.<br>
    <b>Test ID:</b> B307<br>
    <b>Severity: </b>MEDIUM<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py</a><br>
    <b>Line number: </b>188<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_calls.html#b307-eval" target="_blank">https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_calls.html#b307-eval</a><br>

<div class="code">
<pre>
128	
129	            x = m(x)  # run
130	            y.append(x if m.i in self.save else None)  # save output
131	
132	        return x
133	
134	    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency
135	        # https://arxiv.org/abs/1708.02002 section 3.3
136	        m = self.model[-1]  # Detect() module
137	        for mi, s in zip(m.m, m.stride):  # from
138	            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)
139	            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)
140	            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls
141	            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
142	
143	    def _print_biases(self):
144	        m = self.model[-1]  # Detect() module
145	        for mi in m.m:  # from
146	            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)
147	            print((&quot;%6g Conv2d.bias:&quot; + &quot;%10.3g&quot; * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))
148	
149	    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
150	        print(&quot;Fusing layers... &quot;)
151	        for m in self.model.modules():
152	            if isinstance(m, Conv) and hasattr(m, &quot;bn&quot;):
153	                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
154	                delattr(m, &quot;bn&quot;)  # remove batchnorm
155	                m.forward = m.fuseforward  # update forward
156	            elif type(m) is nn.Upsample:
157	                m.recompute_scale_factor = None  # torch 1.11.0 compatibility
158	        return self
159	
160	    def nms(self, mode=True):  # add or remove NMS module
161	        present = isinstance(self.model[-1], NMS)  # last layer is NMS
162	        if mode and not present:
163	            print(&quot;Adding NMS... &quot;)
164	            m = NMS()  # module
165	            m.f = -1  # from
166	            m.i = self.model[-1].i + 1  # index
167	            self.model.add_module(name=str(m.i), module=m)  # add
168	            self.eval()
169	        elif not mode and present:
170	            print(&quot;Removing NMS... &quot;)
171	            self.model = self.model[:-1]  # remove
172	        return self
173	
174	    def autoshape(self):  # add autoShape module
175	        print(&quot;Adding autoShape... &quot;)
176	        m = AutoShape(self)  # wrap model
177	        copy_attr(m, self, include=(&quot;yaml&quot;, &quot;nc&quot;, &quot;hyp&quot;, &quot;names&quot;, &quot;stride&quot;), exclude=())  # copy attributes
178	        return m
179	
180	
181	def parse_model(d, ch):  # model_dict, input_channels(3)
182	    anchors, nc, gd, gw = d[&quot;anchors&quot;], d[&quot;nc&quot;], d[&quot;depth_multiple&quot;], d[&quot;width_multiple&quot;]
183	    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors
184	    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)
185	
186	    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
187	    for i, (f, n, m, args) in enumerate(d[&quot;backbone&quot;] + d[&quot;head&quot;]):  # from, number, module, args
188	        m = eval(m) if isinstance(m, str) else m  # eval strings
189	        for j, a in enumerate(args):
190	            try:
191	                args[j] = eval(a) if isinstance(a, str) else a  # eval strings
192	            except:
193	                pass
194	
195	        n = max(round(n * gd), 1) if n &gt; 1 else n  # depth gain
196	        if m in [
197	            Conv,
198	            Bottleneck,
199	            SPP,
200	            DWConv,
201	            MixConv2d,
202	            Focus,
203	            CrossConv,
204	            BottleneckCSP,
205	            C3,
206	            ShuffleV2Block,
207	            StemBlock,
208	        ]:
209	            c1, c2 = ch[f], args[0]
210	
211	            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2
212	
213	            args = [c1, c2, *args[1:]]
214	            if m in [BottleneckCSP, C3]:
215	                args.insert(2, n)
216	                n = 1
217	        elif m is nn.BatchNorm2d:
218	            args = [ch[f]]
219	        elif m is Concat:
220	            c2 = sum(ch[-1 if x == -1 else x + 1] for x in f)
221	        elif m is Detect:
222	            args.append([ch[x + 1] for x in f])
223	            if isinstance(args[1], int):  # number of anchors
224	                args[1] = [list(range(args[1] * 2))] * len(f)
225	        else:
226	            c2 = ch[f]
227	
228	        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n &gt; 1 else m(*args)  # module
229	        t = str(m)[8:-2].replace(&quot;__main__.&quot;, &quot;&quot;)  # module type
230	        np = sum(x.numel() for x in m_.parameters())  # number params
231	        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, &#x27;from&#x27; index, type, number params
232	        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
233	        layers.append(m_)
234	        ch.append(c2)
235	    return nn.Sequential(*layers), sorted(save)
</pre>
</div>


</div>
</div>

<div id="issue-7">
<div class="issue-block issue-sev-medium">
    <b>blacklist: </b> Use of possibly insecure function - consider using safer ast.literal_eval.<br>
    <b>Test ID:</b> B307<br>
    <b>Severity: </b>MEDIUM<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/78.html" target="_blank">CWE-78</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py</a><br>
    <b>Line number: </b>191<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_calls.html#b307-eval" target="_blank">https://bandit.readthedocs.io/en/1.7.9/blacklists/blacklist_calls.html#b307-eval</a><br>

<div class="code">
<pre>
131	
132	        return x
133	
134	    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency
135	        # https://arxiv.org/abs/1708.02002 section 3.3
136	        m = self.model[-1]  # Detect() module
137	        for mi, s in zip(m.m, m.stride):  # from
138	            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)
139	            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)
140	            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls
141	            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
142	
143	    def _print_biases(self):
144	        m = self.model[-1]  # Detect() module
145	        for mi in m.m:  # from
146	            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)
147	            print((&quot;%6g Conv2d.bias:&quot; + &quot;%10.3g&quot; * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))
148	
149	    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
150	        print(&quot;Fusing layers... &quot;)
151	        for m in self.model.modules():
152	            if isinstance(m, Conv) and hasattr(m, &quot;bn&quot;):
153	                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
154	                delattr(m, &quot;bn&quot;)  # remove batchnorm
155	                m.forward = m.fuseforward  # update forward
156	            elif type(m) is nn.Upsample:
157	                m.recompute_scale_factor = None  # torch 1.11.0 compatibility
158	        return self
159	
160	    def nms(self, mode=True):  # add or remove NMS module
161	        present = isinstance(self.model[-1], NMS)  # last layer is NMS
162	        if mode and not present:
163	            print(&quot;Adding NMS... &quot;)
164	            m = NMS()  # module
165	            m.f = -1  # from
166	            m.i = self.model[-1].i + 1  # index
167	            self.model.add_module(name=str(m.i), module=m)  # add
168	            self.eval()
169	        elif not mode and present:
170	            print(&quot;Removing NMS... &quot;)
171	            self.model = self.model[:-1]  # remove
172	        return self
173	
174	    def autoshape(self):  # add autoShape module
175	        print(&quot;Adding autoShape... &quot;)
176	        m = AutoShape(self)  # wrap model
177	        copy_attr(m, self, include=(&quot;yaml&quot;, &quot;nc&quot;, &quot;hyp&quot;, &quot;names&quot;, &quot;stride&quot;), exclude=())  # copy attributes
178	        return m
179	
180	
181	def parse_model(d, ch):  # model_dict, input_channels(3)
182	    anchors, nc, gd, gw = d[&quot;anchors&quot;], d[&quot;nc&quot;], d[&quot;depth_multiple&quot;], d[&quot;width_multiple&quot;]
183	    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors
184	    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)
185	
186	    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
187	    for i, (f, n, m, args) in enumerate(d[&quot;backbone&quot;] + d[&quot;head&quot;]):  # from, number, module, args
188	        m = eval(m) if isinstance(m, str) else m  # eval strings
189	        for j, a in enumerate(args):
190	            try:
191	                args[j] = eval(a) if isinstance(a, str) else a  # eval strings
192	            except:
193	                pass
194	
195	        n = max(round(n * gd), 1) if n &gt; 1 else n  # depth gain
196	        if m in [
197	            Conv,
198	            Bottleneck,
199	            SPP,
200	            DWConv,
201	            MixConv2d,
202	            Focus,
203	            CrossConv,
204	            BottleneckCSP,
205	            C3,
206	            ShuffleV2Block,
207	            StemBlock,
208	        ]:
209	            c1, c2 = ch[f], args[0]
210	
211	            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2
212	
213	            args = [c1, c2, *args[1:]]
214	            if m in [BottleneckCSP, C3]:
215	                args.insert(2, n)
216	                n = 1
217	        elif m is nn.BatchNorm2d:
218	            args = [ch[f]]
219	        elif m is Concat:
220	            c2 = sum(ch[-1 if x == -1 else x + 1] for x in f)
221	        elif m is Detect:
222	            args.append([ch[x + 1] for x in f])
223	            if isinstance(args[1], int):  # number of anchors
224	                args[1] = [list(range(args[1] * 2))] * len(f)
225	        else:
226	            c2 = ch[f]
227	
228	        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n &gt; 1 else m(*args)  # module
229	        t = str(m)[8:-2].replace(&quot;__main__.&quot;, &quot;&quot;)  # module type
230	        np = sum(x.numel() for x in m_.parameters())  # number params
231	        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, &#x27;from&#x27; index, type, number params
232	        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
233	        layers.append(m_)
234	        ch.append(c2)
235	    return nn.Sequential(*layers), sorted(save)
</pre>
</div>


</div>
</div>

<div id="issue-8">
<div class="issue-block issue-sev-low">
    <b>try_except_pass: </b> Try, Except, Pass detected.<br>
    <b>Test ID:</b> B110<br>
    <b>Severity: </b>LOW<br>
    <b>Confidence: </b>HIGH<br>
    <b>CWE: </b><a href="https://cwe.mitre.org/data/definitions/703.html" target="_blank">CWE-703</a><br>
    <b>File: </b><a href="/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py" target="_blank">/home/c_byrne/tools/sd/sd-interfaces/ComfyUI/custom_nodes/facerestore_cf/facelib/detection/yolov5face/models/yolo.py</a><br>
    <b>Line number: </b>192<br>
    <b>More info: </b><a href="https://bandit.readthedocs.io/en/1.7.9/plugins/b110_try_except_pass.html" target="_blank">https://bandit.readthedocs.io/en/1.7.9/plugins/b110_try_except_pass.html</a><br>

<div class="code">
<pre>
132	        return x
133	
134	    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency
135	        # https://arxiv.org/abs/1708.02002 section 3.3
136	        m = self.model[-1]  # Detect() module
137	        for mi, s in zip(m.m, m.stride):  # from
138	            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)
139	            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)
140	            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls
141	            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
142	
143	    def _print_biases(self):
144	        m = self.model[-1]  # Detect() module
145	        for mi in m.m:  # from
146	            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)
147	            print((&quot;%6g Conv2d.bias:&quot; + &quot;%10.3g&quot; * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))
148	
149	    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers
150	        print(&quot;Fusing layers... &quot;)
151	        for m in self.model.modules():
152	            if isinstance(m, Conv) and hasattr(m, &quot;bn&quot;):
153	                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
154	                delattr(m, &quot;bn&quot;)  # remove batchnorm
155	                m.forward = m.fuseforward  # update forward
156	            elif type(m) is nn.Upsample:
157	                m.recompute_scale_factor = None  # torch 1.11.0 compatibility
158	        return self
159	
160	    def nms(self, mode=True):  # add or remove NMS module
161	        present = isinstance(self.model[-1], NMS)  # last layer is NMS
162	        if mode and not present:
163	            print(&quot;Adding NMS... &quot;)
164	            m = NMS()  # module
165	            m.f = -1  # from
166	            m.i = self.model[-1].i + 1  # index
167	            self.model.add_module(name=str(m.i), module=m)  # add
168	            self.eval()
169	        elif not mode and present:
170	            print(&quot;Removing NMS... &quot;)
171	            self.model = self.model[:-1]  # remove
172	        return self
173	
174	    def autoshape(self):  # add autoShape module
175	        print(&quot;Adding autoShape... &quot;)
176	        m = AutoShape(self)  # wrap model
177	        copy_attr(m, self, include=(&quot;yaml&quot;, &quot;nc&quot;, &quot;hyp&quot;, &quot;names&quot;, &quot;stride&quot;), exclude=())  # copy attributes
178	        return m
179	
180	
181	def parse_model(d, ch):  # model_dict, input_channels(3)
182	    anchors, nc, gd, gw = d[&quot;anchors&quot;], d[&quot;nc&quot;], d[&quot;depth_multiple&quot;], d[&quot;width_multiple&quot;]
183	    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors
184	    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)
185	
186	    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
187	    for i, (f, n, m, args) in enumerate(d[&quot;backbone&quot;] + d[&quot;head&quot;]):  # from, number, module, args
188	        m = eval(m) if isinstance(m, str) else m  # eval strings
189	        for j, a in enumerate(args):
190	            try:
191	                args[j] = eval(a) if isinstance(a, str) else a  # eval strings
192	            except:
193	                pass
194	
195	        n = max(round(n * gd), 1) if n &gt; 1 else n  # depth gain
196	        if m in [
197	            Conv,
198	            Bottleneck,
199	            SPP,
200	            DWConv,
201	            MixConv2d,
202	            Focus,
203	            CrossConv,
204	            BottleneckCSP,
205	            C3,
206	            ShuffleV2Block,
207	            StemBlock,
208	        ]:
209	            c1, c2 = ch[f], args[0]
210	
211	            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2
212	
213	            args = [c1, c2, *args[1:]]
214	            if m in [BottleneckCSP, C3]:
215	                args.insert(2, n)
216	                n = 1
217	        elif m is nn.BatchNorm2d:
218	            args = [ch[f]]
219	        elif m is Concat:
220	            c2 = sum(ch[-1 if x == -1 else x + 1] for x in f)
221	        elif m is Detect:
222	            args.append([ch[x + 1] for x in f])
223	            if isinstance(args[1], int):  # number of anchors
224	                args[1] = [list(range(args[1] * 2))] * len(f)
225	        else:
226	            c2 = ch[f]
227	
228	        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n &gt; 1 else m(*args)  # module
229	        t = str(m)[8:-2].replace(&quot;__main__.&quot;, &quot;&quot;)  # module type
230	        np = sum(x.numel() for x in m_.parameters())  # number params
231	        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, &#x27;from&#x27; index, type, number params
232	        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
233	        layers.append(m_)
234	        ch.append(c2)
235	    return nn.Sequential(*layers), sorted(save)
</pre>
</div>


</div>
</div>

</div>

</body>
</html>
